# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tk1EL3nYNiiQnxOk-6R_7meZjWCb5tXO
"""

!pip install gradio langchain transformers faiss-cpu

!pip install gradio

!pip install langchain faiss-cpu openai flask flask-ngrok

!pip install langchain faiss-cpu flask flask-ngrok sentence-transformers transformers

!pip install -U langchain-community

from langchain.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://internshala.com/student/dashboard")
documents = loader.load()

from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# Use Hugging Face embeddings (no API key required)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Store in FAISS vector database
vectorstore = FAISS.from_documents(docs, embeddings)
vectorstore.save_local("faiss_index")

import gradio as gr
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import pipeline

# Load FAISS vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# Load LLM (small Hugging Face model)
llm_pipeline = pipeline("text-generation", model="facebook/opt-1.3b")
llm = HuggingFacePipeline(pipeline=llm_pipeline)

# Create retrieval-based QA system
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever(search_kwargs={"k": 3}))

# Define chatbot function
def chatbot_response(query):
    return llm_pipeline(query, max_length=50, do_sample=True)[0]['generated_text']

# Gradio UI
iface = gr.Interface(fn=chatbot_response, inputs="text", outputs="text", title="AI Chatbot")

# Launch app
iface.launch(share=True)